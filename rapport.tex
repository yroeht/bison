%\documentclass[a4paper,11pt,final]{article}
% Pour une impression recto verso, utilisez plutôt ce documentclass :
\documentclass[a4paper,11pt,twoside,final]{article}

\usepackage[english,francais]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[pdftex]{graphicx}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage[french]{varioref}
\usepackage{changepage} %for changing margins in appendices
\usepackage{gantt}
\usepackage{float}

\newcommand{\reporttitle}{Rapport de stage}     % Titre
\newcommand{\reportauthor}{Théophile \textsc{Ranquet}} % Auteur
\newcommand{\reportsubject}{Stage de fin de Tronc Commun} % Sujet
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\setlength{\parskip}{1ex} % Espace entre les paragraphes

\hypersetup{pdftitle={\reporttitle},
            pdfauthor={\reportauthor},
            pdfsubject={\reportsubject},
            pdfkeywords={rapport} {vos} {mots} {clés}
}

\begin{document}
  \pagenumbering{roman}
  \include{title}
  \cleardoublepage % Dans le cas du recto verso, ajoute une page blanche si besoin
  \tableofcontents % Table des matières
  \sloppy          % Justification moins stricte : des mots ne dépasseront pas des paragraphes
  \cleardoublepage

  \pagenumbering{arabic}
  \setcounter{page}{1}
  \section*{Introduction} % Pas de numérotation
  \addcontentsline{toc}{section}{Introduction} % Ajout dans la table des matières

  Le stage de 2ème année du cycle d'ingénieur de l'EPITA, aussi appelé stage de
  fin de Tronc Commun, est une expérience critique dans le parcours de chacun.
  On pourrait en dire autant de tout stage, mais celui-ci est le dernier avant
  le stage de fin d'études, qui est un peu différent des autres en cela qu'il
  constitue une sorte de « période d'essai » par les entreprises et débouche
  dans 90\% des cas (chiffres officiels de l'école en 2010) sur une embauche
  (CDI, etc.). Ce stage de fin de tronc commun est donc de fait la dernière
  occasion de découvrir un potentiel métier avant la sortie de l'école, et il
  n'est pas rare que les étudiants choisissent de passer ces cinq mois dans un
  environnement qui n'est pas forcément celui auquel ils se destinaient pour
  leur avenir proche à la fin de leurs études, dans le but de s'assurer qu'ils
  ont fait le bon choix, ou éventuellement de voir si un coup de cœur ne les
  ferait pas changer d'avis.

  Ce stage se place avant les deux semestres de spécialisation des étudiants de
  l'EPITA\@. En ce qui me concerne, j'envisageais avant ce stage de m'orienter
  vers la majeure « Système, Réseau et Sécurité » (SRS). Du coup, il me fallait
  trouver un stage qui me plaise, mais qui me donne une expérience que je
  n'aurai pas forcément l'occasion de reproduire dans un an. Alliant mes
  intérêts pour la programmation Unix en C et C++ d'un côté, et pour la théorie
  des langages et la compilation de l'autre, c'est donc avec un naturel relatif
  que m'est venu l'idée de ce stage.

  Le sujet de ce stage est « contribution aux développements de GNU Bison et
  Tiger », et le lieu est le Laboratoire de Recherche et de Développement de
  l'EPITA (LRDE). C'est un cadre très différent de celui de mon précédent
  stage, et j'espérai par cela pouvoir gagner en maturité quand à la question
  de savoir quel type d'entreprise ou équipe je compte rejoindre à la fin de
  ma scolarité.

  Ce stage, en plus de m'avoir donné la chance de travailler avec excellent
  mentor, m'a permis de travailler avec des technologies auxquelles je n'étais
  pas forcément familier au premier abord, et de contribuer à un projet libre.
  Cela peut paraître anodin, mais c'est en fait tout un savoir faire qu'il faut
  acquérir. J'ai donc gagné avec ce stage un certain nombre de connaissances
  techniques, mais aussi toute une méthodologie qui, pour être tout à fait
  honnête, n'était pas forcément ce par quoi je brillais le plus. J'ai
  également consolidé certaines de mes notions d'informatique fondamentale,
  notamment dans ce qui a trait à l'analyse syntaxique (plus spécifiquement
  l'analyse syntaxique LR avec anticipation) et aux automates associés.

  /* FIXME: 2 more pages. *

  \cleardoublepage
  \section{Présentation du laboratoire}
  \subsection{Présentation générale}

  Le LRDE est un laboratoire de recherche sous tutelle de l'École Pour
  l'Informatique et les Techniques Avancées (EPITA). Il est composé de 8
  chercheurs, 2 administratifs, 3 thésards et 6 étudiants-chercheurs fin 2010.
  Son financement est couvert à 90\% par l'EPITA, le reste venant de contrats
  industriels et de projets académiques.  Appartenant à une école privée, le LRDE
  est une exception dans un monde où la recherche académique scientifique est
  quasi-exclusivement du ressort d'organismes publics.

  Historique Le LRDE a été créé en 1998 à l'initiative de Joël Courtois,
  directeur de l'EPITA, qui désirait équiper l'école d’un véritable laboratoire
  académique, tant pour attirer des enseignants de qualité que pour participer à
  la reconnaissance de l'école par ses pairs. En même temps le laboratoire
  permettrait aux meilleurs élèves de l'école de s'initier au monde de la
  recherche en participant à des projets de recherche.

  En 2005, sous l'impulsion du Conseil Scientifique tout juste crée, la
  nécessité est apparue de structurer le laboratoire autour de thématiques de
  recherche afin de permettre au laboratoire de percer dans le monde académique
  et industriel. La définition de cette thématique a été difficile à trouver.
  Le point commun le plus visible était, et l'est toujours, la programmation
  générique et performante. Il s’agit clairement de la marque de fabrique du
  laboratoire que l'on retrouve dans différents projets du laboratoire et a
  donné lieu à` une dizaine de publication. Aujourd'hui le séminaire du
  laboratoire porte sur cet aspect, mais ce type de programmation est un outil
  au service de projets de recherche qui sont les réelles activités principales
  des chercheurs. Parmi ces activités le traitement d'images était déjà la
  thématique la plus importante du laboratoire mais noyée parmi les autres.
  Aussi après un long processus de réflexion, le LRDE a retenu deux thématiques
  : Reconnaissance des formes, et Automates et vérification.

  \subsection{Les projets}

  Le LRDE a trois projets phares, trois bibliothèques génériques et
  performantes écrites en C++, toutes sous licence libre. Ces bibliothèques
  font partie intégrante de notre re- cherche et peuvent être utilisées dans la
  réalisation de projets et de prestations. Actuelle- ment seule Olena, la
  bibliothèque la plus ancienne du laboratoire et la plus avancée, est utilisée
  dans le cadre de projets et a permis d’obtenir des contrats industriels.

  \subsubsection{Olena – http://olena.lrde.epita.fr/}

  Olena est une plate-forme de traitement d’images générique et performante. Le
  but de cette bibliothèque est de permettre une écriture unique d’algorithmes
  sachant que les entrées de ces algorithmes peuvent être de différente nature.
  Ainsi les entrées possibles sont des images 1D (signaux), 2D (images
  classiques), 3D (volumes), ou des graphes et leurs généralisations (complexes
  cellulaires). De plus, les valeurs stockées dans ces images sont de types
  variés : booléens pour les images binaires, des niveaux de gris avec
  différents encodages, des flottants ou autres. La force d’Olena est de
  préserver la nature abstraite des algorithmes sans pour autant devoir
  sacrifier les performances.

  \subsubsection{Vaucanson – http://vaucanson.lrde.epita.fr/}

  On peut décomposer Vaucanson en trois gros morceaux :
  \begin{enumerate}
    \item Une bibliothèque générique, dans laquelle sont définis
      \begin{enumerate}
        \item différents types de représentation d’automates (graphes ou tables
          de hachage) et d'expressions rationnelles
        \item différents types de monoïdes et semi-anneaux
        \item différents algorithmes qui les manipulent
      \end{enumerate}
    \item Une interface en ligne de commande, appelée TAF-Kit, qui permet
      d’appeler les fonctionnalités principales de la bibliothèque sans devoir
      écrire de C++. Du fait de l'approche générique employée dans la
      bibliothèque, TAF-Kit doit être instanciée pour un type particulier
      (c’est-à-dire un choix de représentation d’automate, monoïde, et semi-
      anneau). Une douzaine de telles instances sont construites pour des types
      prédéfinis.
    \item Une interface graphique dont le développement a été repris par
      l'équipe du Pr. Hsu-Chun Yen à Taïwan.
  \end{enumerate}

  Cette bibliothèque est développée selon un paradigme de programmation
  générique permettant:
  \begin{itemize}
    \item d'écrire les algorithmes une seule fois, indépendamment de la
      structure de données utilisée pour représenter les automates ou les
      expressions rationnelles, et indépendamment du monoïde et du semi-anneau
      utilisé par cet objet.
    \item de ne pas payer le prix de cette abstraction pour être (en théorie)
      aussi rapide à l'exécution qu'un algorithme
  \end{itemize}

  \subsubsection{Spot – http://spot.lip6.fr/}
  Spot est une bibliothèque de \textit{model checking}. C’est-à-dire qu'elle
  offre des algorithmes utiles à la construction d'un \textit{model checker}
  travaillant suivant une approche par automate.  Par rapport aux autres
  outils, nous nous distinguons par l'utilisation d'automates généralisés avec
  conditions d'acceptation sur les transitions, alors que la plupart de la
  recherche est faite sur des automates non-généralisés avec conditions
  d’acceptation sur les états.  Les deux formalismes sont aussi expressifs,
  mais le premier est beaucoup plus concis et permet de réaliser les opérations
  de base de l'approche automate de façon plus efficace:

  \begin{enumerate}
    \item la traduction donne des automates plus compacts, donc le produit est
      (généralement) aussi plus petits
    \item l'utilisation de condition d’acceptation généralisées, permet
      notamment de représenter très facilement des hypothèses d'équité faible
    \item la recherche de circuit acceptant dans un automate généralisé peut se
      faire aussi rapidement que dans un automate non-généralisés.
  \end{enumerate}

  \subsection{Effectifs}

  L'effectif actuel comprend les chercheurs suivants :
  \begin{adjustwidth}{-2cm}{-2cm}
    \begin{center}
      \begin{tabular}{| l | c | l | l | l |}
        \hline
        Nome & Né en & Formation & Statut & Arrivée \\
        \hline
        Ala-Eddine Ben-Salem & 1980 & M2 SLCP (INP Toulouse) & Doctorant &
        2010 \\
        Ana Stefania Calarasanu & & M2 Paris VI & Doctorante & 2012 \\
        Edwin Caralinet & 1990 & EPITA / MVA ENS Cachan & Doctorant & 2012 \\
        Etienne Renault & & M2 Paris VI & Doctorant & 2011 \\
        Yongchao Xu & 1986 & M2 Paris XI & Doctorant & 2010 \\
        \hline
        Guillaume Lazarra & 1985 & EPITA & Ing.\ de recherche & 2008 \\
        \hline
        Réda Dehak  & 1975 & Dr ENST & MdC & 2002 \\
        Akim Demaille & 1970 & X / Dr ENST & MdC & 1999 \\
        Alexandre Duret-Lutz & 1978 & EPITA / Dr Paris VI & MdC & 2007 \\
        Jonathan Fabrizio & 1978 & Dr Paris VI & MdC & 2009 \\
        Thierry Géraud & 1969 & ENST / Dr ENST & MdC & 1998 \\
        Roland Levillain & 1980 & EPITA / M2 SIRF (ENST) & MdC & 2005 \\
        Olivier Ricou & 1966 & Dr Paris VI & MdE - Directeur & 2002 \\
        Didier Verna & 1970 & ENST / Dr ENST & MdC & 2000 \\
        \hline
      \end{tabular}
    \end{center}
  \end{adjustwidth}

  \subsection{La majeure CSI}

  La majeure Calcul Scientifique et Image (CSI) est orientée vers la recherche
  académique et permet à des étudiants de s'immerger dans un laboratoire aux
  côtés des enseignants chercheurs. Les thèmes de recherche vont du traitement
  d'images à la manipulation d'automates en passant par le traitement de la
  parole, le model-checking ou l'aide à la décision.

  L'ingénieur CSI se destine dans une première phase à la préparation d'une
  thèse, en France ou à l'étranger, et il rejoindra ensuite la communauté des
  chercheurs dans un cadre académique ou au sein des structures de recherche de
  grandes entreprises ou de start-up innovantes.\footnote{%
  http://www.epita.fr/cursus-cycle-ingenieur-majeures.aspx}

  Voici le sujet du travail des sept étudiants CSI de la promotion 2013 qui
  étaient à mes côtes pendant ce stage\footnote{%
  http://lrde.epita.fr/cgi-bin/twiki/view/Publications/Seminar-2013-01-16}:

  \subsubsection*{Spot: Réduction par simulation pour les TGBA (Thomas Badie)}

  L'approche par automates du model checking s'appuie traditionnellement sur
  des Automates de Büchi (BA) qu'on souhaite les plus petits possible. Spot,
  bibliothèque de model checking, utilise principalement des TGBA qui
  généralisent les BA\@. Nous avons déjà présenté une méthode de réduction par
  simulation (dite directe). Cette technique a permis de produire des automates
  plus petits que dans les précédentes versions.  La simulation consiste à
  fusionner les états ayant le même suffixe infini. Nous montrons que nous
  pouvons aussi fusionner ceux ayant le même préfixe infini (c'est la
  cosimulation). On peut répéter la simulation et la cosimulation pour créer la
  simulation itérée. Cette méthode est incluse dans Spot 1.0 et est une grande
  amélioration de la simulation.  On expérimente aussi une méthode qui consiste
  à modifier certaines conditions d'acceptations (appellées sans importances).
  Puisque celles qui sont sur les transitions entre composantes fortement
  connexes n'ont pas d'influence sur le langage, on peut les modifier pour
  aider la simulation.

  \subsubsection*{Spot: Méthodes de réduction par ordre partiel adaptatives
  (Pierre Parutto)}

  Le model checking explicite de systèmes concurrents souffre d'une croissance
  exponentielle du nombre d'états représentant un système.  Les méthodes de
  réduction par ordre partiel sont un ensemble de méthodes permettant de
  combattre ce problème. Celles-ci permettent d'ignorer les états redondants
  lors de la génération de l'espace d'états. Parmi elles, nous avons choisi les
  algorithmes two phase et ample set comme base pour nos investigations.
  Ceux-ci ont été implémentés dans Spot, la bibliothèque C++ de model checking
  développée au LRDE, en utilisant l'interface DiVine. En se basant sur ces
  méthodes et sur le fait que les algorithmes dans Spot sont calculés à la
  volée, nous avons défini une nouvelle classe de méthodes appelées méthodes de
  réduction par ordre partiel adaptatives. L'idée est de se baser sur l'état
  courant de l'automate de la formule et non sur la formule tout entière. Les
  résultats obtenus sur notre suite de tests montrent que cette méthode donne
  de meilleurs résultats que les
  méthodes d'ordre partiel classiques.

  \subsubsection*{Speaker ID: Spherical Discriminant Analysis (Victor Lenoir)}

  Le rôle de la vérification de locuteur est de vérifier l'identité présumée
  d'un segment de parole. Actuellement, les meilleures performances sont
  obtenues par un mapping de chaque segment de parole d'un locuteur vers un
  vecteur appelé I-vector. Le score de la vérification de locuteur est calculé
  par une distance cosine entre ces deux vecteurs représentant chacun un
  locuteur. Ce rapport décrit une technique de réduction de dimension appelée
  Spherical Discriminant Analysis (SDA). Les objectifs de cette projection sont
  de maximiser la distance cosinus entre deux locuteurs différents et de
  minimiser la distance cosinus entre deux même locuteurs; il a été montré que
  le sous-espace de la SDA, qui est plus approprié pour la distance cosinus que
  la Linear Discriminant Analysis (LDA), obtient de meilleures performances en
  reconnaissance faciale. Nous allons comparer les performances obtenues par la
  SDA avec celles obtenues par la LDA.

  \subsubsection*{Climb: Parallélisation de Climb (Laurent Senta)}

  Climb est une bibliothèque de traitement d'image générique développée en
  Common Lisp. Elle fournit une couche de généricité bas niveau utilisée pour
  définir de nouveaux algorithmes de traitement d'image. Il est aussi possible
  de créer des chaînes de traitements soit en utilisant un "Domain Specific
  Language" déclaratif ou bien à l'aide d'une interface graphique. Afin
  d'améliorer les performances de la bibliothèque, ces différents niveaux
  peuvent être parallélisés. Nous décrirons les différents aspects de ce
  processus de parallélisation. Pour chaque niveau, nous détaillons
  l'implémentation des traitements parallèles, leurs impacts sur
  l'utilisabilité de la bibliothèque ainsi que les gains de performance
  obtenus.

  \subsubsection*{Vaucanson: FSMXML pour Vaucanson 2.0 (David Moreira)}

  Vaucanson est une bibliothèque de manipulation d'automates et de
  transducteurs. La version 2.0 est aujourd'hui en cours de développement et le
  design a été revu pour avoir des parties statiques et dynamiques. Dans
  Vaucanson 1.4, les entrées/sorties utilisent intensivement le format XML
  spécifié par le groupe de Vaucanson, FSMXML\@. Mes travaux consistent à
  développer et rafraîchir des spécifications du format présent dans Vaucanson
  1.4. Cette mise à jour nous permet la sauvegarde et la lecture d'automates
  aux Weight Sets particuliers tels que des expressions rationnelles ou même
  des automates pondérés.

  \subsubsection*{Olena: Calcul du flux optique dans des séquences avec des
  parties manquantes (Sylvain Lobry)}

  Calculer le flux optique peut être un premier pas vers l'inpainting vidéo.
  Pour cette application, nous devons manipuler des séquences avec des zones
  manquantes, celles à inpainter. Le flux optique peut être calculé de manière
  locale ou globale. Les méthodes globales ont généralement de meilleurs
  résultats. Dans le cas de séquences avec des zones manquantes, les méthodes
  globales ne peuvent pas être utilisées de manière directe à cause du manque
  d'informations dans ces régions. Nous présentons une méthode combinant des
  algorithmes locaux et globaux afin de calculer le flux optique dans ce type
  de séquences ce qui nous permet d'inpainter efficacement et simplement des
  vidéos.

  \subsubsection*{Olena: Variational image inpainting by combination of features
  (Coddy Levi)}

  L'inpainting consiste à réparer des parties d'une image de façon visuellement
  plausible. Une catégorie de méthodes répondant à ce problème est basée sur
  des équations aux dérivées partielles (EDP). Cette approche consiste à
  propager itérativement des informations géométriques et d'intensités à
  l'intérieur des régions à réparer. Cependant l'élaboration de tels modèles
  demande une compréhension théorique du processus de diffusion souvent
  difficile à obtenir. Basé sur le papier de Risheng Liu traitant d'une
  approche ascendante à la composition de l'EDP par combinaison d'invariants
  simples, nous proposons une mise en œuvre optimisée que nous comparons à
  l'existant.

  \subsection{Positionnement du stage au sein du laboratoire}

  Bison est un projet GNU, ce n'est pas un projet du laboratoire. Cependant,
  son mainteneur est Akim Demaille, membre permanent du LRDE\@. De part la
  nature du projet Bison\footnote{voir section suivante}, ce stage était très
  proche de la Théorie des Langages, qui n'est pas étrangère au LRDE non plus.

  Enfin, un des projets du LRDE est le compilateur Tiger\footnote{%
  http://www.lrde.epita.fr/~akim/ccmp/tiger.html}, qui utilise Bison, et
  une partie de mon travail visait très spécifiquement des fonctionnalités
  développées avant tout pour Tiger (car non publiées).

  \cleardoublepage

  \section{Le projet GNU Bison}

  GNU Bison est l'implémentation de l'analyseur syntaxique yacc par le projet
  GNU.

  \subsection{L'ancêtre YACC}

  Le programme YACC est un générateur de parseurs développé par Stephen C.
  Johnson pour UNIX en 1970, chez AT\&T Corporation. Ce nom est un acronyme
  récursif pour \textit{Yet Another Compiler Compiler}.

  Pour faire tourner les parseurs (analyseurs sémantiques) générés, il faut
  faire appel à un analyseur lexical, souvent généré avec des outils tels Lex.
  Le standard IEEE POSIX P1003\footnote{%
  http://pubs.opengroup.org/onlinepubs/009695399/utilities/yacc.html} décrit
  les fonctionnalités et requis pour ces deux programmes.

  Lex est capable de traiter des langages de type 3 (réguliers), et yacc
  fournit le code nécessaire à l'analyse de langages de type 2
  (non-contextuels); les parseurs (les analyseurs sémantiques) générés sont
  LALR.

  \subsection{Analyseurs syntaxiques}

  L'entrée \textit{input} de tout programme informatique a une structure. En
  fait, un programme peut être vu comme définissant un "langage d'entrée" qu'il
  accepte. Un langage d'entrée peut être complexe, par exemple un langage de
  programmation, ou simple, tel une simple séquence de nombre. Malheureusement,
  les méthodes canoniques d'entrée/sortie utilisateur sont limitées, difficiles
  d'utilisation, et souvent laxistes quant à la vérification de la validité de
  l'entrée.\footnote{http://dinosaur.compilertools.net/yacc/index.html}

  Yacc fournit un outil générique pour décrire l'entrée d'un programme.
  L'utilisateur spécifie les structures de son entrée, avec le code à invoquer
  pour chaque structure reconnue. Yacc transforme ces spécifications en une
  routine qui gère tout le processus de lecture de l'entrée; il est
  d'usage de déléguer toute l'interaction avec l'entrée à cette routine.

  Cette routine produite par YACC appelle à une autre routine, fournie par
  l'utilisateur, pour retourner le prochain élément atomique de l'entrée; on
  parle de \textit{token} (lexème). Ainsi, l'utilisateur peut choisir le niveau
  d'abstraction qu'il souhaite dans son \textit{input}: traiter les caractères
  au plus bas niveau, un par un, ou alors avec des notions plus haut niveau:
  mots, nombres, etc. Cette routine peut également gérer des idiomes, tels les
  commentaires.

  YACC est écrit en C portable, et la classe de spécifications acceptée est
  très large: des grammaires LALR(1) désambigüisées.

  En plus de l'écriture de compilateurs C, APL, Pascal, RATFOR, etc., YACC a
  également utilisé pour des des langages moins conventionnels, par exemple un
  deboggeur FORTRAN.

  La grammaire est une collection de règles. Chaque règle décrit une structure
  autorisée et la nomme. Par exemple, une règle pourrait être:

  \begin{verbatim}
  date : month_name day ',' year ;
  \end{verbatim}

  Ici, \texttt{date}, \texttt{month\_name}, \texttt{day}, et \texttt{year}
  représentent des structures d'intérêt particulier dans l'entrée; on suppose
  que \texttt{month\_name}, \texttt{day}, et \texttt{year} sont définis
  ailleurs. La virgule ',' est entre guillements; ceci signifie que la virgule
  apparait telle quelle dans l'entrée. Les deux points et le point virgule
  ne sont là que pour ponctuer la règle elle-même et n'ont aucune valeur
  vis-à-vis de l'entrée. Ainsi, avec les définitions qui vont bien, la règle
  ci-dessus filtre (reconnait) l'entrée suivante:

  \begin{verbatim}
  July 4, 1776
  \end{verbatim}

  \subsection{Présentation de bison}

  Le projet GNU est un projet informatique dont les premiers développements ont
  été réalisés en janvier 1984 par Richard Stallman pour développer le système
  d'exploitation GNU\@. Le projet est maintenu par une communauté de hackers
  organisée en sous-projets. Chaque brique du projet est un logiciel libre
  utilisable de par sa nature dans des projets tiers, mais dont la finalité est
  de s'inscrire dans une logique cohérente avec l'ensemble des sous-projets en
  vue de la réalisation d'un système d’exploitation complet et entièrement
  libre, et avec pour stratégie, l'utilisation de l'existant.

  Le projet Bison a été initié en 1986 par Richard M. Stallman, qui souhaitait
  développer une implémentation GNU de YACC\@. Il apporte comme nouveautés un
  certain nombre de nouvelles directives, une nouvelle façon de typer les
  productions des règles, le support de nouveaux langages (C++ et Java), ainsi
  que la génération des parserus GLR.

  Voici un exemple relativement typique de fichier que l'on pourrait vouloir
  donner en entrée à Bison:

  \begin{verbatim}
%skeleton "lalr1.cc"
%define variant
%debug

%token <int> ELEM
%type < std::list<int> > exp
%token TEOF 0

%printer { yyo << $$; } <int>
%printer
    {
      for (std::list<int>::const_iterator i = $$.begin (); i != $$.end (); ++i)
        {
          if (i != $$.begin ())
            yyo << ", ";
          yyo << *i;
        }
    } < std::list<int> >

%code requires { #include <list> }
%code { int yylex (yy::parser::semantic_type* yylval); }

%%
exp: /* empty */    { /* magic*/ }
   | exp[li] ELEM { $li.push_front ($ELEM); $$ = $li; }
%%

void yy::parser::error (std::string const& msg)
{
  std::cerr << msg << std::endl;
}
  \end{verbatim}

  Ce à quoi on décide par exemple de joindre notre fonction d'analyse lexicale
  (elle pourrait aussi être générée par Flex, dans un autre fichier donc):

  \begin{verbatim}
int yylex (yy::parser::semantic_type* yylval)
{
  static int input = 0;
  if (input++ == 10)
    return yy::parser::token::TEOF;
  else
    {
      yylval->build (input);
      return yy::parser::token::ELEM;
    }
}
  \end{verbatim}

  Ainsi qu'un point d'entrée pour faire tourner tout ceci:

  \begin{verbatim}
int main ()
{
  yy::parser p;
  p.set_debug_level (1);
  p.parse ();
}
  \end{verbatim}

  On distingue trois sections, séparées par des \og \%\% \fg:

  \begin{enumerate}
    \item Le prologue, qui contient des \textit{directives} Bison (qui
      commencent toutes par \%), dans notre cas:
      \begin{itemize}
        \item \texttt{\%skeleton} choisit quel squelette de parseur on veut
            utiliser. Ceci n'implique rien concernant le type de parseur
            généré: par défault c'est du LALR, et utiliser le squelette
            \texttt{glr.cc} génèrera un LALR dans un squelette LALR\@. Pour
            générer un parseur GLR, il faut utiliser la directive
            \texttt{\%glr-parser}. Ici, on génère simplement un parseur C++
            LALR\@.
        \item \texttt{\%define variant} est une nouveauté dans Bison, et
            remplace l'ancien \texttt{\%union}. Cette directive permet
            d'utiliser un variant en lien d'un union, ce qui a pour avantage
            de permettre de stocker objets, ce qui n'est pas possible dans un
            union en C++ (on ne peut y stocker que de la
            \textit{plain old data}, POD). Ainsi on peut utiliser des
            objets pour représenter la valeur sémantique des productions de
            la grammaire, très agréable par exemple lorsque l'on construit
            un \textit{arbre de syntaxe abstraite} (AST).
        \item \texttt{\%type} et \texttt{\%token} définissent les terminaux
            (\textit{tokens}) présents dans la grammaire. Une valeur leur est
            associée, et c'est cette valeur qui est retournée par l'analyseur
            lexical \textit{yylex} à chaque mot, et qui permet de savoir quel
            type de token a été filtré.
        \item \texttt{\%printer}, qui définit comment afficher les
            différentes valeurs sémantiques des productions de la grammaire
            dans les traces de debug.  Par exemple, l'affichage des listes
            n'est pas immédiat, il est donc nécessaire de fournir la routine
            nécessaire.
        \item \texttt{\%code} définit le code à insérer avant le corps du
            squelette du parser: typiquement les prototypes des fonctions
            fournies par l'utilisateur.
      \end{itemize}
    \item La grammaire.
    \item L'épilogue, qui contient les fonctions requises par les parseurs
      générés, à savoir:
      \begin{itemize}
        \item L'analyseur lexical \texttt{yylex}.
        \item La routine de traitement d'erreur \texttt{error}.
      \end{itemize}
  \end{enumerate}

  \subsection{Age et volumétrie du projet}

  Globalement, GNU Bison c'est 5 201 commits réalisés par 33 contributeurs sur
  une période de 26 ans pour un total de 49 968 lignes de code. Le numéro de
  version à mon arrivée était 2.6.2, autrement dit le projet était déjà mature.
  Il n'y avait de bugs de longue date à réparer, juste un certain nombre de
  possibilités d'améliorations et d'ajout de \textit{features} intéressantes.

  \begin{figure}[H]
    \begin{center}
      \begin{tabular}{| r | c | c |}
        \hline
        Lignes & Total & Pourcentage \\
        \hline
        code & 33 650 & 67.3\% \\
        commentaire & 8 392 & 16.8\% \\
        vide & 6 926 & 15.9\% \\
        \hline
      \end{tabular}
      \caption{Parts des commentaires dans le code source}
    \end{center}
  \end{figure}

  Bison est écrit en C90 (70\% du volume). Les parseur sont générés à partir de
  squelettes, qui sont remplis avec des tables correspondant à la grammaire de
  ce cas précis d'utilisation ainsi qu'avec les action données par
  l'utilisateur. Ces squelettes sont écris dans le langage cible (donc C, C++,
  et Java), mais articulé avec des macros GNU M4.

  Il y a quelques templates XSLT qui sont fournis avec Bison afin de pouvoir
  générer les sorties (texte, html, et graphviz) à partir d'un fichier XML.

  La testsuite est écrite en M4 aussi. Enfin, il y a des outils de
  developpement, par exemple les scripts de benchmarking, en Perl. Au total,
  en comptant le Texinfo de la documentation, le Bison des parseurs internes et
  le Lex des scanneurs internes de Bison, ce sont dix langages qui sont
  représentés\footnote{données provenant de http://www.ohloh.net/p/bison/}.

  \begin{figure}[H]
    \begin{adjustwidth}{-2cm}{-2cm}
      \begin{center}
        \includegraphics[scale=0.55]{images/language-breakdown}
        \caption{Parts des différents langages de programmation dans le projet}
      \end{center}
    \end{adjustwidth}
  \end{figure}

  Sur ces 12 derniers mois, il y a eu 891 commits (ce qui est 720 de plus que
  l'année précédente, soit une augmentation de 421\%), de 8 contributeurs (un
  de plus que l'an dernier). Voici un graphique qui donne un aperçu de
  historique de l'activité sur le projet depuis sa création:

  \begin{figure}[H]
    \begin{adjustwidth}{-2cm}{-2cm}
      \begin{center}
        \includegraphics[scale=0.55]{images/commits-per-month}
        \caption{Évolution des commits par mois depuis 1988}
      \end{center}
    \end{adjustwidth}
  \end{figure}

  On voit bien les débuts inactifs (jusqu'à 1993). En effet, le travail n'était
  alors versionné que de façon très basique, avec RCS (Revision Control
  System). En 1993, le travail collaboratif commence. Il y a un pic d'activité
  en 2001, lorsqu'Akim et d'autres nouveaux contributeurs arrivent (notamment
  des étudiants de l'EPITA), et apportent de gros changements à l'architecture
  du projet: création d'un répertoire lib/ qui contient des implémentations
  utiles au projet, d'une suite de tests, et surtout mise en place du backend
  actuel en M4. Depuis ces années 2000, il y a entre 2 et 5 contributeurs
  actifs chaque année (donc plus qu'avant 2002)

  \begin{figure}[H]
    \begin{adjustwidth}{-2cm}{-2cm}
      \begin{center}
        \includegraphics[scale=0.55]{images/line-count}
        \caption{Évolution du nombre de lignes (de code, de commentaire, et
          vides) depuis 1988.}
      \end{center}
    \end{adjustwidth}
  \end{figure}

  \begin{figure}[H]
    \begin{adjustwidth}{-2cm}{-2cm}
      \begin{center}
        \includegraphics[scale=0.55]{images/contributors}
        \caption{Part des contributions des différents mainteneurs dans le
        projet actuel.}
      \end{center}
    \end{adjustwidth}
  \end{figure}
  \cleardoublepage

  \section{Travail effectué}

  \subsection{Vue d'ensemble}

  Mon stage a débuté septembre 2012. Peu avant moi, en juin 2012, Victor
  Santet (EPITA promotion 2015) était venu y faire un mois de stage. Il est
  parti en laissant des bases intéressantes pour une poursuite du travail dans
  sa lancée. Le travail en question portait sur les avertissements et erreurs
  générés par Bison. J'ai donc commencé mon stage en continuant sur cette
  lancée, et ce pendant un mois. Cette tâche correspond à l'élément [1] dans le
  diagramme de Gantt fourni plus bas.

  La continuation a --là encore-- été guidé très largement par les suggestions
  de mon mentor.  Il y avait déjà une liste d'améliorations possibles maintenue
  dans un fichier TODO, ce fut une source d'inspiration assez facile pour
  débuter.

  C'est ainsi que la suite de mon travail porta sur une option non essentielle
  de Bison: \texttt{-{}-graph}, qui génère une visualisation de l'automate utilisé
  par l'analyseur généré. Ce graphe était relativement pauvre, et je l'ai donc
  enrichi. Suite à diverses complications, cette tâche pris environ un mois et
  demi. Cette phase de développement se découpa en deux phases bien distinctes,
  j'ai donc pris le soin de les séparer ci-après. Le lecteur les retrouvera aux
  indices [3] et [4].

  Vinrent à ce moment plusieurs nouvelles pistes d'amélioration, lancées par
  Akim. La première ([5]) était la résolution d'un "bug" (en fait c'est n'est
  pas vraiment un, comme je l'explique plus loin dans ce rapport) de longue
  date qui faisait que si l'utilisateur ne spécifiait pas d'argument
  supplémentaire pour l'interface de la fonction de rapport d'erreur alors dans
  certains squelettes les informations de localisation de l'erreur étaient
  absentes (mais dans d'autres si). La deuxième était une amélioration de
  l'affichage des erreurs ([2]). En effet, le choix avait été fait de respecter
  des conventions semblables à celles de GCC (\textit{GNU Compiler
  Collection}), et donc l'introduction récente de changements dans le rapport
  des erreurs par celui-ci méritait une imitation dans Bison. J'ai travaillé
  sur ces deux aspects là en parallèle (bien que, contrairement à ce qu'on
  pourrait croire en retenant que ces deux fonctionnalités portent sur les
  messages d'erreurs, celles-ci étaient tout à fait orthogonales).

  Ce fut alors le premier jalon de ce stage: la sortie de Bison 2.7, qui
  incluait mes contributions de ces 3 premiers mois de stage.

  Fort des connaissances acquises, je commençai alors à travailler sur les
  squelettes de Bison.

  J'implémentai alors quelques modifications aux squelettes C++ GLR ([8]) et
  LALR ([9]) mais je me suis heurté à ce qu'on pourrait appeler un mur: le
  schéma utilisé pour représenter les symboles n'était pas du tout viable si on
  voulait envisager des améliorations du genre de ce que je proposais (et qui,
  honnêtement, paraissent maintenant bien mineures par rapport à l'avalanche de
  travail engendré par ce qui a suivi). On a donc du ensuite retravailler le
  fonctionnement des symboles ([10]).

  \begin{adjustwidth}{-4cm}{-2cm}
  \begin{gantt}[xunitlength=0.7cm,fontsize=\small,titlefontsize=\small,drawledgerline=true]{14}{20}
    \begin{ganttitle}
      \titleelement{2012}{16}
      \titleelement{2013}{4}
    \end{ganttitle}
    \begin{ganttitle}
      \numtitle{9}{1}{12}{4}
      \numtitle{1}{1}{1}{4}
    \end{ganttitle}
    \begin{ganttitle}
      \numtitle{1}{7}{22}{1}
      \numtitle{1}{7}{22}{1}
      \numtitle{1}{7}{22}{1}
      \numtitle{1}{7}{22}{1}
      \numtitle{1}{7}{22}{1}
    \end{ganttitle}
    \ganttbar{warnings as errors [1]}{1}{4}
    \ganttbarcon{caret diagnostics [2]}{10}{2}
    \ganttbar{show reductions -g [3]}{5}{2}
    \ganttbarcon{show reductions xslt [4]}{7}{4}
    \ganttbar{api.pure full [5]}{10}{2}
    \ganttmilestonecon{version 2.7 released}{14}
    \ganttbar{leak hunting [7]}{13}{2}
    \ganttbar{glr.cc [8]}{15}{1}
    \ganttbarcon{lalr1.cc [9]}{16}{1}
    \ganttbarcon{symbols [10]}{17}{3}
  \end{gantt}
  \end{adjustwidth}

  \cleardoublepage

  \subsection{Erreurs et avertissements}

  Bison est un compilateur, il va donc tenter de construire un fichier en
  sortie à partir d'un fichier en donné en entrée par l'utilisateur. Du coup,
  rien ne garantit la validité de celui-ci. Il peut y avoir des informations
  manquantes ou incompatibles dans les données lues par Bison. De fait, les
  compilateurs sont des outils de travail avec lesquels l'utilisateur interagit
  beaucoup, et en pratique le développeur (qui est plus habitué à ce que les
  choses se passent mal que à ce qu'elles se passent bien) est d'ailleurs
  souvent plus intéressé par les problèmes rencontrés par le compilateur que
  par la sortie en assembleur elle-même: la gestion des erreurs est donc un
  aspect très important des compilateurs en général, et ceux-ci incluent Bison.

  La compatibilité avec YACC (c'est à dire le comportement décrit par l'IEEE
  1003\footnote{%
  http://pubs.opengroup.org/onlinepubs/009695399/utilities/yacc.html}) n'impose
  absolument rien sur l'affichage à produire dans le terminal en cas d'erreur.
  Le choix a donc été fait de suivre le comportement de GCC, qui distingue très
  nettement plusieurs catégories d'erreurs:

  \begin{itemize}
    \item \textit{warnings}, les simples avertissements à l'utilisateur. Ils
      indiquent que le compilateur a détecté quelque chose de suspect dans le
      code de l'utilisateur, potentiellement une erreur de sa part. Le
      programme peut tout de même continuer son exécution et produire un
      résultat plausible.
    \item \textit{complaints}, ou tout simplement \textit{errors}, les erreurs
      que le compilateur rencontre sur un bout du code de l'utilisateur. Le
      programme peut continuer à traiter le reste du code de l'utilisateur,
      et aller le plus loin possible dans son exécution avant d'être bloqué
      inconditionnellement. La compilation ne produit pas de fichier en sortie.
    \item \textit{fatal errors}, les erreurs pour lesquelles ça n'aurait aucun
      sens de continuer l'exécution plus loin. Le programme s'arrête
      immédiatement.
  \end{itemize}

  \vspace{0.5cm}

  Voici la façon dont GCC les signale, par exemple:

  \begin{verbatim}
test.c: In function ‘main’:
test.c:6:3: error: expected declaration or statement at end of input
test.c:4:9: warning: unused variable ‘ar’ [-Wunused-variable]
  \end{verbatim}

  On remarque que les messages commencent par \textit{error:} ou
  \textit{warning:} selon
  qu'il s'agisse d'une erreur ou d'un simple avertissement. On note également
  que les avertissement sont divisés en de nombreuses catégories. Par exemple,
  ici, l'avertissement est attaché à la catégorie \textit{unused-variable},
  comme indiqué en fin du message.
  Voici un extrait du manuel de GCC qui montre le type de catégories de qu'il
  existe:

  \begin{verbatim}
Warning Options
       -fsyntax-only  -fmax-errors=n  -pedantic -pedantic-errors -w -Wextra
       -Wall  -Waddress -Waggregate-return  -Warray-bounds -Wno-attributes
       -Wno-builtin-macro-redefined -Wc++-compat -Wc++11-compat -Wcast-align
       -Wcast-qual -Wchar-subscripts -Wclobbered  -Wcomment -Wconversion
       -Wcoverage-mismatch  -Wno-cpp -Wno-deprecated
  \end{verbatim}

  Ces options servent à activer les avertissements associés. Il est d'usage de
  n'en activer que quelques uns par défaut et de laisser à l'utilisateur le
  soin de sélectionner quelles catégories d'avertissements il veut que le
  compilateur lui rapporte, ainsi que lesquelles il veut au contraire qu'il
  ignore.

  Le travail de Victor, sur lequel je me suis basé, avait ajouté le support de
  cette organisation des avertissements de Bison en catégories, activables
  indépendamment.

  Voici un extrait du guide d'utilisation de Bison:

  \begin{verbatim}
Operation modes:
  -h, --help                 display this help and exit
  -V, --version              output version information and exit
      --print-localedir      output directory containing locale-dependent data
      --print-datadir        output directory containing skeletons and XSLT
  -y, --yacc                 emulate POSIX Yacc
  -W, --warnings[=CATEGORY]  report the warnings falling in CATEGORY

Warning categories include:
  `midrule-values'    unset or unused midrule values
  `yacc'              incompatibilities with POSIX Yacc
  `conflicts-sr'      S/R conflicts (enabled by default)
  `conflicts-rr'      R/R conflicts (enabled by default)
  `deprecated'        obsolete constructs
  `other'             all other warnings (enabled by default)
  `all'               all the warnings
  `no-CATEGORY'       turn off warnings in CATEGORY
  `none'              turn off all the warnings
  `error[=CATEGORY]'  treat warnings as errors
  \end{verbatim}

  Et voici une démonstration de leur usage (notez la ressemblance avec le
  comportement de GCC):

  \begin{verbatim}
  $ bison -Wdeprecated
  input.yy:2.9-15: warning: deprecated directive: ‘%define variant’, use
  ‘%define api.value.type variant’ [-Wdeprecated]
  input.yy:3.4-5: warning: empty character literal [-Wother]

  $ bison -Wno-deprecated
  input.yy:3.4-5: warning: empty character literal [-Wother]

  $ bison -Wnone
  (n'affiche rien)
  \end{verbatim}

  Notez que Bison active par défaut un certain nombre de catégories, dont
  \textit{deprecated} et \textit{other}.


  \subsubsection{Avertissements de dépréciation}

  A propos de ces avertissements de dépréciation, Victor avait préparé le
  terrain pour leur utilisation, en rendant les routines de rapport d'erreurs
  génériques non seulement entre erreurs et avertissements, mais également
  entre avertissements de catégories différentes. Cependant, les avertissements
  de dépréciation n'étaient jamais émis. La dépréciation n'était alors que
  documentée, et notée en commentaire dans le code de Bison. Une de mes tâches
  fut donc, pour générer les avertissements de l'exemple précédent,  de
  transformer ainsi\footnote{commit 2062d72, Thu Oct 18 18:00:51 2012} les
  \textit{scanner} et \textit{parser} de Bison-même, car c'est à ce niveau que
  les directives déprécies sont lues par le programme, et qu'il est toujours
  intéressant de gérer les choses le plus tôt possible pour s'en débarrasser
  pour la suite

  \begin{verbatim}
diff --git a/src/parse-gram.y b/src/parse-gram.y
index f0187fb..1624dde 100644
--- a/src/parse-gram.y
+++ b/src/parse-gram.y
@@ -317,7 +317,6 @@ prologue_declaration:
 | "%expect" INT                    { expected_sr_conflicts = $2; }
 | "%expect-rr" INT                 { expected_rr_conflicts = $2; }
 | "%file-prefix" STRING            { spec_file_prefix = $2; }
-| "%file-prefix" "=" STRING        { spec_file_prefix = $3; } /* deprecated */
 | "%glr-parser"
 (..)
diff --git a/src/scan-gram.l b/src/scan-gram.l
index 8e48148..95edacc 100644
--- a/src/scan-gram.l
+++ b/src/scan-gram.l
+#define DEPRECATED(Msg)                                         \
+  do {                                                          \
+    size_t i;                                                   \
+    complain (loc, Wdeprecated,                                 \
+              _("deprecated directive: %s, use %s"),            \
+              quote (yytext), quote_n (1, Msg));                \
+    scanner_cursor.column -= mbsnwidth (Msg, strlen (Msg), 0);  \
+    for (i = strlen (Msg); i != 0; --i)                         \
+      unput (Msg[i - 1]);                                       \
+  } while (0)
+
(...)
+  /* deprecated */
+  "%default"[-_]"prec"              DEPRECATED("%default-prec");
+  "%error"[-_]"verbose"             DEPRECATED("%define parse.error verbose");
+  "%expect"[-_]"rr"                 DEPRECATED("%expect-rr");
+  "%file-prefix"{eqopt}             DEPRECATED("%file-prefix");
  \end{verbatim}

  Il y a plusieurs choses intéressantes à noter, outre le fait que maintenant
  cet avertissement soit effectivement généré.

  Notez que dans le \textit{parser}, les constructions dépréciées n'étaient pas
  traitées spécialement, ni regroupées, mais juste discrètement annotées. Par
  souci de concision, je n'ai gardé qu'une seul telle ligne ci-dessus, mais il
  y en avait trois dans ce fichier. J'ai déplacé la reconnaissance de ces
  motifs un cran plus bas, dans le \textit{scanner}-même (le plus tôt, le
  mieux c'est), où neuf autres directives dépréciées se situaient, et qui elles
  n'étaient même pas marquées comme telles. Ces directives sont maintenant
  regroupées dans un paragraphe, et utilisent tous une même macro, pour
  faciliter le travail des futurs mainteneurs.

  Dans la macro, on remarque l'appel à la fonction \texttt{complain}, par
  laquelle passe tout message d'erreur ou d'avertissement, sur laquelle j'ai
  également travaillé (détaillé plus loin).

  Également intéressant, et fruit de mon travail, la présence de \texttt{unput}
  et de la soustraction sur le curseur de position: lorsqu'une directive
  invalide dépréciée est lue, on la supprime du flux et on recommence la
  lecture en ayant inséré à la place la bonne directive (du coup le problème est
  réglé pour le reste du programme, qui peut continuer sans avoir ni à gérer
  cette directive -{}- ce qui irait à l'encontre du but de la dépréciation
  -{}-, ni à contenir du code dupliqué) mais également en ayant pris soin de ne
  pas corrompre les information de position des lexèmes, indispensables aux
  facilités de correction des bugs pour l'utilisateur, ce que aurait eu lieu si
  on avait remplacé une directive dépréciée par une autre plus longue de $n$
  caractères, Bison signalant ainsi à l'utilisateur toute erreur plus loin sur
  la même ligne comme étant décalée de $n$.

  Voici par exemple à quoi ressemblent des informations de position erronées
  par cette correction de la dépréciation:

  \begin{verbatim}
input.y:13.1-14: warning: deprecated directive, use '%define parse.error
verbose' [-Wdeprecated]
 %error_verbose %error_verbose
 ^^^^^^^^^^^^^^
input.y:13.16-29: warning: deprecated directive, use '%define parse.error
verbose' [-Wdeprecated]
 %error_verbose %error_verbose
                ^^^^^^^^^^^^^^
input.y:13.11-21: error: %define variable 'parse.error' redefined
 %error_verbose %error_verbose
           ^^^^^^^^^^^
 input.y:13-6:         previous definition
 %error_verbose %error_verbose
      ^
  \end{verbatim}

  Notez le \textit{caret diagnostic} du troisième message qui souligne quelque
  chose qui ne correspond pas à l'erreur.

  Mais ce cas est en fait bien plus vicieux. Ce bug est toujours présent à ce
  jour, d'ailleurs (alors que ceux que j'évoquais précédemment ne le sont plus,
  mais ressemblaient exactement à ceci, et j'évite en trichant ainsi avec les
  exemples de multiplier les exemples) Comme la nouvelle façon de demander de
  la verbosité est de définir la variable \texttt{parse.error}, et qu'on a
  effectué cette substitution plusieurs fois, on se retrouve avec une variable
  redéfinie.  L'erreur se situe dans le code de substitution, pas dans le code
  de l'utilisateur, notre système de localisation des erreurs perd donc
  totalement les pédales et \underline{ment}. D'ailleurs, le lecteur attentif
  aura constaté que le problème de dépréciation, qui relève du \textit{warning}
  a généré une \textit{error}, c'est très gênant car cela implique que si
  l'utilisateur avait fait le choix d'invoquer Bison avec l'option
  \texttt{-Wno-deprecated}, il n'aurait pas ces avertissements pour lui mettre
  la puce à l'oreille, il aurait juste une erreur, avec une \textit{location}
  \underline{fausse} (ce qui aurait été le cas également avec une erreur
  présente dans le code de l'utilisateur, mais au moins il aurait eu une chance
  de l'y trouver alors que là elle n'existe juste pas).

  Le lecteur vraiment attentif, lui, aura remarqué le dernier message, qui lui
  commence différemment des autres: ni par \textit{error}, ni par
  \textit{warning}. Il s'agit d'une information de contexte.

  \subsubsection{Informations de contexte, et préfixage}

  Les compilateurs fournissent souvent à l'utilisateur des informations
  supplémentaires concernant les erreurs, pour aider l'utilisateur. Voici un
  exemple avec gcc-4.7\footnote{%
  http://gcc.gnu.org/wiki/ClangDiagnosticsComparison}:

  \begin{verbatim}
  deduce.cc: In function 'void g()':
  deduce.cc:6:10: error: no matching function for call to 'f(A&)'
  deduce.cc:6:10: note: candidate is:
  deduce.cc:1:24: note: template<class T> void f(typename T::type)
  deduce.cc:1:24: note:   template argument deduction/substitution failed:
  deduce.cc: In substitution of 'template<class T> void f(typename T::type)
  [with T = A]':
  deduce.cc:6:10:   required from here
  deduce.cc:1:24: error: no type named 'type' in 'struct A'
  \end{verbatim}

  Il y a trois type de messages dans cet affichage qui ne sont pas des erreurs,
  mais qui sont simplement des indications supplémentaires.

  \begin{itemize}
    \item Le premier, \og \textit{In function `void g()'} \fg permet de
      localiser l'erreur d'une façon plus significative pour l'utilisateur. Il
      n'y a pas vraiment de situation dans Bison où l'on aurait matière à
      préciser quelque chose de genre.
    \item Le deuxième, \og \textit{deduce.cc:1:24: note: template<class T> void
      f(typename T::type)} \fg ce sont les messages qui commencent par
      \textit{note}, et qui sont des précisions complémentaires, souvent des
      indices quant à des façons de résoudre le problème. Là encore, Bison n'a
      pas grand chose de très intéressant à ajouter, les erreurs elles-mêmes
      étant généralement plutôt explicites.
    \item Enfin, la ligne qui va nous intéresser est celle qui lit \og
      \textit{deduce.cc:6:10:   required from here}\fg. Le message qui précède
      fait référence à plusieurs \textit{location}, or il est d'usage de donner
      celles-ci avec ce format très particulier \texttt{fichier:ligne:colonne}
      en début de ligne, il est donc naturel de donner les \textit{location}
      supplémentaires via des messages additionnels. Le corps de ces messages
      est indenté par rapport aux précédents.
  \end{itemize}

  Dans Bison, le cas des erreurs faisant référence à un bout du code situé en
  amont est fréquent. C'est par exemple le cas des variables (re)définies avec
  \texttt{\%define}, ou des tokens dont le numéro est en conflit, comme ici:

  \begin{verbatim}
r.y:10.10-22: error: user token number 112 redeclaration for HEX_1
r.y:9.8-16:       previous declaration for DECIMAL_1
  \end{verbatim}

  Ce second message est indenté, dans l'esprit de la remarque précédente, pour
  mettre en évidence la nature contextuelle de l'information.

  Cette indentation est un produit de mon travail. Auparavant, une routine
  existait pour afficher un message indenté: il s'agit de
  \texttt{complain\_at}.  Cette routine ne servait malheureusement que pour
  certains messages, celui ci-dessus par exemple utilisait la simple procédure
  \texttt{complain}, et n'était aucunement indentée. Voici un exemple des
  lègères corrections qui furent nécessaire à la bonne mise en forme de ces
  messages\footnote{commit cbaea01, Wed Sep 26 11:49:19 2012}:

  \begin{verbatim}
 static void
 semantic_type_redeclaration (semantic_type *s, const char *what, location first,
                              location second)
 {
-  complain_at (second, _("%s redeclaration for <%s>"), what, s->tag);
-  complain_at (first, _("previous declaration"));
+  unsigned i = 0;
+  complain_at_indent (second, &i, _("%s redeclaration for <%s>"), what, s->tag);
+  i += SUB_INDENT;
+  complain_at_indent (first, &i, _("previous declaration"));
 }
  \end{verbatim}

  Le lecteur assidu aura noté la présence d'une fonction \og \texttt{\_} \fg,
  qui était également présente dans le précédent extrait de code. Il s'agit en
  fait d'un \textit{wrapper} autour de la fonction de traduction\footnote{%
  http://translationproject.org/html/welcome.html} \texttt{gettext}, définit
  comme suit:

  \begin{verbatim}
  src/system.h: # define \_(Msgid)  gettext (Msgid)
  \end{verbatim}

  Comme les messages sont traduit selon une collection de motifs,, il faut bien
  faire attention à ne pas les découper n'importe comment, sinon on rend le
  travail des traducteurs difficile. A propos des traductions délicates, voici
  un exemple de message étrangement traduit:

  \begin{verbatim}
  $ LC_ALL=en_US.utf8 bison -Wdeprecated t.y
  t.y:5.1: error: rule given for a, which is a token
  $ LC_ALL=fr_FR.utf8 bison -Wdeprecated t.y
  t.y:5.1: erreur: la règle pour a, qui est un terminal
  \end{verbatim}

  Pour en revenir à notre exemple, on peut s'intéresser à l'implémentation de
  cette fonction \texttt{complain\_at}. Comme on peut le voir, on appelle aussi
  cette fonction pour la partie non-indentée du message. En fait, on initialise
  un entier à zéro, et on le passe à notre fonction par référence afin qu'elle
  soit modifiée. Lorsque cette valeur est nulle, la fonction se comporte comme
  la simple \texttt{complain}, au détail près qu'elle modifie cette valeur pour
  y stocker la colonne courante du début du message. Lors du second appel, la
  valeur n'est plus nulle, et \texttt{complain\_at} a un nouveau comportement:
  elle affiche le corps du message  à la colonne correspondant à la valeur
  stockée dans la variable. L'incrément manuel de cette valeur entre les deux
  appels correspond donc au changement de niveau d'indentation. Cette interface
  peu sembler un peu compliquée au premier abord: on aurait pu se contenter
  d'ajouter un Booléen indiquant le corps du message était à indenter ou non.
  Ce que l'on gagne avec cette méthode est la gestion des niveaux d'indentation
  imbriqués, au cas où un jour où en aurait l'utilité.

  Une faiblesse de cette approche (qui aurait également été présente dans la
  version plus simple que je viens de suggérer) est que l'on part du postulat
  que la partie gauche du second message (la \textit{location}) aura la même
  taille que celle du premier message. C'est généralement vrai, la différence
  potentielle se limitant généralement à quelques colonnes, et généralement
  dans le sens négatif (c'est à dire que la seconde est plus courte que la
  première, conséquence directe du fait que ce second message fait à priori
  référence à une déclaration précédente, et donc avec un numéro de ligne et/ou
  colonne inférieur --donc à potentiellement moins de chiffres), mais dans le
  cas contraire, la différence empièterait sur l'indentation. Exemple adapté du
  précédent:

  \begin{verbatim}
r.y:1.1-2: error: user token number 112 redeclaration for HEX_1
r.y:1000.18-26:previous declaration for DECIMAL_1
  \end{verbatim}

  Notez que le \textit{previous:} est à la même position relativement au
  \textit{error:}, mais que du fait du changement de taille de la
  \textit{location} l'indentation est maintenant absente.

  Ce cas ne devrait jamais se produire (la seule situation plausible étant non
  pas une différence dans les numéros de ligne mais de colonne, or on imagine
  difficilement un scénario raisonnable à mille colonnes), mais il existe une
  façon de le gérer\footnote{Déjà évoquée sur la mailing-liste:
  http://lists.gnu.org/archive/html/bison-patches/2009-09/msg00086.html}: il
  faudrait utiliser un \textit{buffer} pour stocker les messages, en y insérant
  des marqueurs là où on souhaite changer le niveau d'indentation, et ensuite
  effectuer une deuxième passe pour effectuer les insertions de blancs.

  Enfin, dernier point d'intérêt sur ce sujet: le préfixe des erreurs. En
  effet, le lecteur a très certainement constaté que nos messages d'erreurs
  commencent par \textit{error:} tandis que les avertissement, eux, commencent
  par \textit{warning:}. C'est explicite, et ça semble très naturel. Pourtant,
  ce sont des additions récentes: Victor a préfixé les avertissements, et c'est
  moi même qui ai pris la responsabilité de préfixer les erreurs.

  \subsubsection{\textit{warnings as errors}}

  Une fonctionnalité remarquable de GCC est d'activer le traitement des
  avertissements en erreurs pour certaines catégories. Par exemple, avec
  l'option \texttt{-Werror=unused-argument}, les arguments d'une fonction qui
  ne sont pas utilisés par celle-ci sont traités comme des erreurs.

  Maintenant que Bison dispose de catégories de d'avertissements, il était
  devenu aisé d'implémenter ce comportement, et ce fut en fait la première
  tâche qui m'avait été assignée (bien que j'ai été un peu distrait en cours de
  route par d'autres considérations d'ergonomie sur les erreurs, détaillées
  précédemment).

  L'option \texttt{-Werror}, permettant de promouvoir la totalité des
  avertissements en erreurs, existait déjà. En fait, c'était un hack: cette
  option active une fausse catégorie \textit{error} d'avertissements qui sert,
  dans la routine de génération des avertissements et erreurs, à promouvoir
  ceux-ci en ces dernières lorsqu'elle est activée. Ceci n'offre aucune
  flexibilité, c'est donc une impasse. Il a fallu repenser la façon de faire.

  Une grande contrainte dans l'ajout du support d'une nouvelle option dans la
  ligne de commande est que l'on veut au maximum s'intégrer dans les fonctions
  de reconnaissance d'arguments actuelles. En l'occurrence, Bison dispose de
  routines génériques et assez élégantes pour reconnaitre des consructions de
  la forme \texttt{-Wno-deprecated,other,no-yacc -{}-report=state,solved} pour
  factoriser \texttt{-Wno-deprecated, -Wother -Wno-yacc -{}-report=state
  -{}-report=solved}. On note au passage le support des variantes \texttt{no-}
  de chaque catégorie, qui se fait sans la moindre duplication de code.

  Voici un extrait de la fonction \texttt{flags\_argmatch} telle qu'elle
  existait à mon arrivée:

  \begin{verbatim}
      args = strtok (args, ",");
      while (args)
        {
          int no = strncmp (args, "no-", 3) == 0 ? 3 : 0;
          int value = XARGMATCH (option, args + no, keys, values);

          /* operations sur warn_flags ici */
  \end{verbatim}

  Selon la valeur de \texttt{no}, le traitement sur la variable globale
  \texttt{warn\_flags} stockant (sous forme de bits) les \textit{flags} activés
  est différent: il peut s'agit d'une mise d'un bit à 1, ou à 0.

  J'ai décidé de m'inspirer de cette façon de faire. J'ai introduit une
  nouvelle globale, un véritable miroir de \texttt{warn\_flags}, que j'ai nommé
  \texttt{error\_flags}, et qui stocke de la même façon l'information \og est
  ce que cet avertissement est à traiter en erreur? \fg. Du coup, l'unique
  différence entre les façons de traiter \texttt{-W[no-]category} et
  \texttt{-W[no-]error=category} est la variable à modifier: dans un cas, on
  veut modifier la reconnaissance d'une catégorie, dans l'autre cas on veut
  activer son passage en erreur.

  Le résultat se voit dans ces quelques lignes\footnote{%
  commit 20964c3, Mon Oct 1 15:01:03 2012}:

  \begin{verbatim}
    for (args = strtok (args, ","); args; args = strtok (NULL, ","))
      {
        size_t no = STRPREFIX_LIT ("no-", args) ? 3 : 0;
        size_t err = STRPREFIX_LIT ("error", args + no) ? 5 : 0;

        flag_argmatch (option, keys,
                       values, all, err ? &errors_flag : flags,
                       args, no, err);
      }
  \end{verbatim}

  Ce bout de code constitue en fait le corps de la nouvelle fonction
  \texttt{flags\_argmatch}. Les opérations sur les variables globales ont été
  déplacées dans une nouvelle fonction, pour une meilleur segmentation du code.

  Voici ces fameuses opérations, avec des explications pas à pas:

  \begin{itemize}
    \item Première partie de l'alternative (\texttt{-W[no-]category}): ces
      opérations font de simples opérations bits à bits: un masque
      (\texttt{\&=$\sim$}) dans le cas du \texttt{no-}, afin de
      désactiver le bit courant tout en préservant l'état des autres bits; un
      OU (\texttt{|=}) dans le cas normal, pour mettre le bit en question à un.


  \begin{verbatim}
    if (value)
      {
        if (no)
          *flags &= ~value;
        else
          {
  \end{verbatim}

    \item Il y a une action en plus à effectuer dans le
      cas où on fait du \texttt{error=}. En effet, par choix (cohérent avec
      celui de GCC), on veut que \texttt{-Werror=category} ne fasse pas
      qu'activer la promotion de cette catégorie en erreur, mais également
      qu'il active les \textit{warnings} de cette catégorie. Sinon,
      l'utilisateur se retrouve à devoir spécifier deux fois ce nom de
      catégorie: une fois pour le \texttt{-W}, et une fois pour le
      \texttt{-Werror=}. Ainsi, il faut travailler sur les deux variables en
      même temps.

  \begin{verbatim}
            if (err)
              warnings_flag |= value;
            *flags |= value;
          }
      }
  \end{verbatim}

    \item Deuxième partie de l'alternative (\texttt{-W[no-]none}): cette valeur
      est spéciale: plutôt que signifier la modification d'une catégorie
      précise, elle affecte au contraire toutes les autres (et éventuellement
      elle-même, cette catégorie n'ayant aucune utilité en soi pour le reste
      des évènements): c'est à dire que lorsqu'on \textit{set} \texttt{none},
      en fait ce qu'on veut c'est \textit{reset} tout le reste, d'où le code
      qui reprend le cas "usuel", mais inversé. Notons que la catégorie
      \texttt{all} est gérée encore différemment: c'est une catégorie un peu
      comme les autres, sauf que sa valeur dans l'énumération C des catégories
      (qui sont des bits: 1, 2, 4, 8, etc.) est \texttt{$\sim$all}, c'est à
      dire une valeur qui correspond "magiquement" à l'union de toutes les
      autres valeurs possibles de l'énumération.

  \begin{verbatim}
    else
      {
        if (no ? !err : err)
          *flags |= all;
        else
          *flags &= ~all;
      }
  \end{verbatim}


    \item Le mystérieux ternaire code le comportement suivant: de même que
      \texttt{-Wno-none} signifie \texttt{-Wall},
      \texttt{-Wno-error=none}\footnote{%
        Le lecteur qui a du mal à s'y retrouver peut imaginer ceci: on précise
        que, pour aucun avertissement, il ne faut pas promouvoir ceux-ci en
        erreur; c'est à dire qu'on donne une précision qui ne s'applique à
      rien, et qui n'a donc aucune importance.}, signifie
      \texttt{-Werror=all}\footnote{Il faut bien comprendre que le "error=" ne
        change pour ainsi dire rien, si ce n'est la variable courante: on peut
        donc "simplifier" ce terme de l'option, et le résultat devient
      immédiat, puisqu'on vient de rappeler que no-none est équivalent à all.}
      ce qui est faux. L'usage de \texttt{-Wno-error=category} n'activant pas
      implicitement le
      \texttt{-Werror} des autres catégories, il n'y a aucune raison pour que
      le \texttt{-Wno-error=none} le fasse.
    \item Les constructions fantaisistes à la \texttt{-Wno-error=no-category}
      sont impossibles, de par le fonctionnement du \textit{wrapper}
      \texttt{flags\_argmatch}.
  \end{itemize}


  Résultat final:


  \begin{verbatim}
  $ bison -Werror=deprecated
  input.yy:2.9-15: error: deprecated directive: ‘%define variant’, use ‘%define
  api.value.type variant’ [-Werror=deprecated]
  input.yy:3.4-5: warning: empty character literal [-Wother]
  \end{verbatim}

  \subsubsection{\textit{caret diagnostics}}

  Une caractéristique très appréciée de Clang par la communauté est sa façon de
  rapporter les erreurs, en adjoignant à chacune une ligne du code source
  fautif correspondant, en soulignant l'endroit de l'erreur. C'est ce qu'on
  appelle des \textit{caret diagnostics}, ou encore des \textit{caret errors}.

  Ils sont introduits dans gcc-4.8 également. Ca ressemble à ceci\footnote{%
  http://gcc.gnu.org/wiki/ClangDiagnosticsComparison}:

  \begin{verbatim}
$ gcc-4.8 -fsyntax-only t.c

t.c: In function 'f':
t.c:6:11: error: invalid operands to binary / (have '__m128' and 'const int *')
   myvec[1]/P;
           ^
$ clang-3.1 -fsyntax-only t.c

t.c:6:11: error: can't convert between vector values of different size ('__m128'
and 'int const *')
  myvec[1]/P;
  ~~~~~~~~^~
  \end{verbatim}

  Bison ne dispose pas actuellement d'assez d'informations pour un affichage
  aussi évolué que celui de Clang, mais nos \textit{location} sont tout à fait
  suffisantes pour imiter le comportement de GCC\@. En fait, comme l'exemple le
  montre, GCC se contente souvent de montrer la colonne de début du lieux
  intéressant, tandis que dans Bison nous avons des informations un peu plus
  riches: on garde trace de la colonne (et de la ligne) de début, mais aussi de
  fin. Par exemple, dans l'exemple suivant, on voit des \textit{location} d'une
  largeur de quatre colonnes.

  La question s'est posée de savoir comment optimiser la routine d'affichage de
  ces \textit{carets}:
  \begin{verbatim}
$ bison -fcaret input.y

input.y:10.13-17: error: %destructor redeclaration for foo
 %destructor {baz} "foo"
             ^^^^^
input.y:5.13-17:      previous declaration
 %destructor {bar} foo
             ^^^^^
\end{verbatim}

  On voit que nous avons fait le choix de souligner l'intégralité du lieux, et
  pas juste la première colonne. On aurait pu vouloir tenter une imitation de
  Clang, et utiliser \verb|^~~~|, mais comme on aurait jamais pu
  mettre l'accent sur autre chose que le premier caractère, l'intérêt n'était
  pas flagrant.


  \subsubsection{La suite de tests}

  Pour chacun de ces tests, il a fallu maintenir la suite de tests, afin de
  s'assurer de la non-régression du projet. La politique suivie est très
  simple: tout ce qui est visible de l'utilisateur doit être testé. C'est donc
  le cas pour les messages d'erreurs, sur lesquels nous avons travaillé.

  Afin d'être le plus efficace possible, certains tests sont exécutés plusieurs
  fois: une fois avec le traitements des warnings en erreurs, et une fois sans.
  Pour éviter toute duplication de code (ou plutôt, de sortie attendue), on ne
  spécifie que la sortie attendue avec les warnings, c'est ensuite une macro de
  la testsuite qui se charge de transformer cette sortie en l'équivalent avec
  \texttt{--graph}. Par exemple, il s'agit d'adapter


  \begin{verbatim}
  input.yy:3.4-5: warning: empty character literal [-Wother]
  \end{verbatim}

  en

  \begin{verbatim}
  input.yy:3.4-5: error: empty character literal [-Werror=other]
  \end{verbatim}

  Les habitués auront reconnu là un cas d'application assez banal de
  \texttt{sed}. Cepedandant, comme l'on se situe dans une macro M4 (la suite de
  tests n'est composée que de ça), il faut prendre quelques complications en
  compte. Du coup, la commande que l'on aurait voulu exécuter:

  \begin{verbatim}
  $PERL -pi -e 's{(.*): warning: (.*)\[-W(.*)\]$}
                $1:error $2[-Werror=$3]}'             expout
  \end{verbatim}

  ne peut pas être utilisée telle quelle. En effet, en M4, \texttt{\$N} est
  l'expansion de l'argument $n$ de la macro. Il faut donc \og l'échapper \fg
  ainsi: \texttt{\$][N}. Et immédiatement, un deuxième problème survient, de
  Perl lui-même cette fois ci: \texttt{\$N[]} est vu comme un
  déréférencement de tableau, et non pas comme \og le motif $n$ suivi de
  crochets\fg. Il faut donc échapper ces crochets. Enfin, le niveau
  d'imbrication des crochets étant devenu problématique côté M4, il a fallu les
  remplacer par les trigraphes associes, à savoir \texttt{@<:@} et \texttt{@:>}
  pour \texttt{[} et \texttt{]} respectivement. On finit donc avec le
  magnifique:

  \begin{verbatim}
  $PERL -pi -e 's{(.*): warning: (.*)\[-W(.*)\]$}
                $1:error $][2\@<:@-Werror=$][3@:>@}'  expout
  \end{verbatim}

  La mise à jour du M4 n'a été qu'une succession de créations artistiques du
  genre. Pour citer Akim, \textit{M4 \_is\_ sexy, et les crochets c'est
  tellement plus beau que des parenthèses\ldots}

  \cleardoublepage

  \subsection{Visualisation graphique}

  Derrière les parseurs générés par Bison se cachent en fait des automates à
  décalage et à réduction. Pour illustrer nos propos, prenons une grammaire
  canonique (mais conflictuelle):

  \begin{verbatim}
%%
exp: a | b
a:
b:
  \end{verbatim}

  Il y a un conflit \textit{reduce/reduce} (R/R), c'est à dire que pour un même état
  l'automate peut réduire selon deux règles différentes: ici, lorsqu'on
  reconnait le mot vide, on peut aussi bien réduire selon \texttt{a} que selon
  \texttt{b}. L'automate n'a aucun moyen raisonable de choisir (ce qui aurait
  pu être le cas en cas de présence de \textit{lookahead token (LAT)}, de mot de
  regard en avant), il y a donc conflit. Notez le choix de ce type de conflit
  pour l'exemple, et non pas d'un \textit{shift/réduce}: dans ce cas,
  l'automate résoud lui-même le conflit en privilégiant le décalage, et la
  réduction est donc écartée de l'automate (et ne serait pas visible sur le
  graphe qui suit). En cas de conflit R/R, c'est la première réduction (dans
  l'ordre de la grammaire) qui est choisie; la deuxième n'étant jamais utilisée
  on la marque donc comme inutile.

  Ces informations sont déjà disponibles sous un format textuel donc voici un
  extrait:

  \begin{verbatim}
State 0

    0 $accept: . exp $end
    1 exp: . a
    2    | . b
    3 a: .  [$end]
    4 b: .  [$end]

    $end      reduce using rule 3 (a)
    $end      [reduce using rule 4 (b)]
    $default  reduce using rule 3 (a)

    exp  go to state 1
    a    go to state 2
    b    go to state 3
    \end{verbatim}

    La réduction inutile est marquée entre crochets. Le nouvel affichage des
    réductions est:

  \begin{figure}[H]
    \begin{adjustwidth}{-2cm}{-2cm}
      \begin{center}
        \includegraphics[scale=0.55]{images/badgraph}
        \caption{Représentation de l'automate sous-jacent de la grammaire
        input.y, Bison 2.7}
      \end{center}
    \end{adjustwidth}
  \end{figure}

  Afin de mieux apprécier mon travail, voici ce qui existait auparavant:

  \begin{figure}[H]
    \begin{adjustwidth}{-2cm}{-2cm}
      \begin{center}
        \includegraphics[scale=0.55]{images/old}
        \caption{Représentation de l'automate sous-jacent de la grammaire
        input.y, Bison 2.6}
      \end{center}
    \end{adjustwidth}
  \end{figure}

  Une première partie du travail a été l'embellissment via des choix plus
  cohérents: bulles carrées, police monospace, précision de \texttt{State N} en
  lieu d'un simple \texttt{N}.

  Une deuxième partie du travail a été l'amélioration de l'affichage des règles
  de grammaires relatives à chaque état: l'ancien affichage n'était pas
  cohérent avec celui de la sortie texte, elle est maintenant identique.

  Enfin, les réductions sont maintenant visibles.

  \subsubsection{Affichage des réductions}

  On aurait pu croire que la présence d'un affichage des réductions dans la
  sortie texte, avec une marque pour les réductions rendues inutiles par un
  conflit, et leur LAT associé, était un indicateur d'immédiateté de la tâche
  que nous souhaitions: afficher les réductions "normales" en vert, avec leur
  LAT comme étiquette (sauf pour les actions par défaut, par souci de
  lisibilité); et les réductions désactivées par conflit non résolu par
  l'utilisateur en rouge.

  Malheureusement (pour les fainéants, \textit{heureusement} pour les
  enthousiastes tels le jeune stagiaire que j'étais), cela n'a pas été si
  facile. En effet, l'information \og réduction désactivée \fg, ainsi que
  l'information \og il n'y a pas de LAT qui ne correspond pas à l'action par
  défaut pour cette réduction \fg, n'est pas accessible facilement. Cela passe
  par la construction d'une collection de données (un \textit{bitset}, en
  l'occurrence).

  L'extrait de sortie texte montre quelque chose de très désagréable: les
  réductions sont traitées par LAT (ici \texttt{\$end}) d'abord, \textit{puis}
  par règle de grammaire. Or, pour notre sortie Graphviz, on veut afficher un
  arc par règle, mais avec tous les LAT associés à la règle: on veut effectuer
  le traitement dans l'ordre inverse. Il a donc fallu comprendre l'algorithme,
  et l'adapter à ce nouveau sens. Les détails n'intéresseront pas le lecteur,
  mais nous en avons tiré une bonne connaissance de l'implémentation des
  réductions.

  Voici le code produit, qui construit les \textit{potentiels} arcs:

  \begin{verbatim}

  /* Two obstacks are needed: one for the enabled reductions, and one
     for the disabled reductions, because in the end we want two
     separate edges, even though in most cases only one will actually
     be printed. */
     (...)
      /* Build the lookahead tokens lists, one for enabled transitions and one
         for disabled transistions. */
      if (default_reduction && default_reduction == reds->rules[j])
        defaulted = true;
      if (reds->lookahead_tokens)
        {
          int i;
          for (i = 0; i < ntokens; i++)
            if (bitset_test (reds->lookahead_tokens[j], i))
              {
                if (bitset_test (no_reduce_set, i))
                  firstd = print_token (&dout, firstd, symbols[i]->tag);
                else
                  {
                    if (! defaulted)
                      firste = print_token (&eout, firste, symbols[i]->tag);
                    bitset_set (no_reduce_set, i);
                  }
              }
        }
  \end{verbatim}

  Les commentaires sont explicites. Tout d'abord, il y a les considérations
  relevant du caractère \og action par défaut \fg: ces réductions ne doivent
  pas être étiquetées par le moindre LAT (ce afin de facilement distinguer le
  cas par défaut). Ce qui explique le \texttt{if (! defaulted)}: si la
  réduction courante correspond à une réduction par défaut active, alors on ne
  rajoute pas de potentiels autres LAT à cet arc.

  Le \textit{bitset} \texttt{no\_reduce\_set} permet de (re-)déterminer
  l'activation ou non d'une réduction. L'information n'est pas conservée
  jusqu'à cet endroit du programme, car elle est tout à fait inutile pour le
  fonctionnement de Bison. On ne souhaite pas que le programme dans son
  intégralité soit impacté par ce genre de fonctionnalité.

  Il serait intéressant d'étudier des possibilités pour ne pas dupliquer le
  calcul de cette donnée dans les cas où on en a besoin plusieurs fois.

  Le backend de l'affichage des réductions relève de la jongle entre
  \textit{obstacks}. En effet, deux sont systématiquement construites: il faut
  les différencier (pour ne pas que les arcs soient fusionnés), et les afficher
  comme on le souhaite, mais ne pas afficher du tout les arcs dont l'obstack
  est vide. Ici encore les commentaires sont relativement explicites:

  \begin{verbatim}
  /* If no lookahead tokens were valid transitions, this reduction is
     actually hidden, so cancel everything. */
  if (first)
    (void) obstack_finish0 (out);
  else
    {
      char const *ed = enabled ? "" : "d";
      char const *color = enabled ? ruleno ? "3" : "1" : "5";

      /* First, build the edge's head. The name of reduction nodes is "nRm",
         with n the source state and m the rule number. This is because we
         don't want all the reductions bearing a same rule number to point to
         the same state, since that is not the desired format. */
      fprintf (fout, "  %1$d -> \"%1$dR%2$d%3$s\" [",
               source, ruleno, ed);

      /* (The lookahead tokens have been added to the beginning of the
         obstack, in the caller function.) */
      if (! obstack_empty_p (out))
        {
          char *label = obstack_finish0 (out);
          fprintf (fout, "label=\"[%s]\", ", label);
          obstack_free (out, label);
        }

      /* Then, the edge's tail. */
      fprintf (fout, "style=solid]\n");
  \end{verbatim}

  Les couleurs retenues pour les \og diamants \fg de notre représentation sont
  bien assorties car sont issues d'un thème prédéfini de Graphviz\footnote{%
  http://www.graphviz.org/doc/info/colors.html}.

  \subsubsection{Adaptation à la production via XML}

  Il a été pensé, vers 2006, qu'il serait intéressant d'introduire un format
  de sortie plus riche que le simple texte: un schéma XML de l'automate. Bison
  est fourni avec des feuilles de transformation XSLT (\textit{Extensible
  Stylesheet Language Transformations}). Ce langage exploite toute la puissance
  du XML, notamment sa nature propice au \textit{pattern matching}.

  \begin{figure}[H]
    \begin{center}
      \includegraphics[scale=0.55]{images/xslt}
      \caption{Fonctionnement de XSLT, tiré de Wikipedia, l'encyclopédie
      libre.}
    \end{center}
  \end{figure}

  Le fichier \texttt{xml2text.xsl} permet de générer une copie exacte de la
  sortie textuelle à partir du simple \texttt{.xml}, le fichier
  \texttt{xml2html.xsl} fait de même pour la sortie HTML, \ldots et le fichier
  \texttt{xml2dot.xsl} de même pour la sortie Graphviz.

  Il a donc fallu adapter ce fichier de transformations pour obtenir une copie
  exact de nos nouveaux graphes: sinon, on perdait la compatibilité (et
  l'équivalence) entre ces deux méthodes de production, et cela aurait été une
  régression.

  Recoder cette création de \textit{bitset} aurait été pénible en XSLT\@.
  Heureusement, la richesse du fichier XML produit nous évita bien du mal. En
  voici un extrait:

  \begin{verbatim}
    <state number="0">
      (...)
      <actions>
        (...)
        <reductions>
          <reduction symbol="$end" rule="3" enabled="true"/>
          <reduction symbol="$end" rule="4" enabled="false"/>
          <reduction symbol="$default" rule="3" enabled="true"/>
        </reductions>
      </actions>
      <solved-conflicts/>
    </state>
  \end{verbatim}

  Le point qui doit retenir notre attention ici est la présence d'un attribut
  \texttt{enabled}, qui nous facilite grandement la tâche. La procédure est
  simple:

  \begin{enumerate}
    \item On traite une fois chaque règle pour laquelle on a un arc à générer,
      c'est-à-dire deux fois si elle est présente une fois (ou plus) avec un
      \texttt{enabled} vrai et une fois (ou plus) avec cet attribut faux.
    \item Pour chaque couple (\textit{numéro de règle, activation}) ainsi
      obtenu, on réitère sur l'ensemble des réductions de cet état, afin de
      récupérer tous les attributs \texttt{symbol} pour lesquels on a ce
      couple. On obtient ainsi la liste des LAT avec laquelle on va étiquetter
      notre arc.
  \end{enumerate}

  Le code XSLT correspondant à ce premier filtrage:

  \begin{verbatim}
<xsl:for-each select='reduction'>
    <!-- These variables are needed because the current context can't be
         refered to directly in XPath expressions. -->
    <xsl:variable name="rul">
      <xsl:value-of select="@rule"/>
    </xsl:variable>
    <xsl:variable name="ena">
      <xsl:value-of select="@enabled"/>
    </xsl:variable>
    <!-- The foreach's body is protected by this, so that we are actually
         going to iterate once per reduction rule, and not per lookahead. -->
    <xsl:if test='not(preceding-sibling::*[@rule=$rul and @enabled=$ena])'>
  \end{verbatim}

  Et au second:

  \begin{verbatim}
        <xsl:for-each select='../reduction[@enabled=$ena and @rule=$rule]'>
  \end{verbatim}

 On retrouve également notre gestion du cas particulier des actions par
  défaut:
  \begin{verbatim}
      <!-- Don't show labels for the default action. In other cases, there will
           always be at least one token, so 'label="[]"' will not occur. -->
      <xsl:if test='$rule!=0 and not(../reduction[@enabled=$ena and @rule=$rule and @symbol="$default"])'>
  \end{verbatim}

  Globalement, on constate que la technologie XPath nous a bien rendu service,
  même si pour le premier filtrage celle-ci s'est avérée trop limitée. On
  aurait en fait eu besoin du \texttt{group-by} de XSLT 2.0, mais Bison utilise
  encore 1.0, et il est vrai que quelques lignes supplémentaires ne justifient
  pas la nécessité d'une version plus récente pour les utilisateurs.

  \cleardoublepage

  \subsection{Squelettes}

  Les années 2000 ont vu le remplacement de l'ancien \texttt{bison.hairy} par
  un backend à bases de squelettes articulés par des macros GNU M4. Cela permet
  de très facilement mettre des alternatives selon qu'on directive a été
  spécifiée ou non. Par exemple, la directive \texttt{\%define parse.assert}
  cause l'émission d'un certain nombre d'assertions qui seraient sinon
  absentes.

  Cela se fait de la sorte (la macro \texttt{b4\_parse\_assert\_if} étant
  définie en amont):

  \begin{verbatim}
    template <typename T>
    T&
    build ()
    {]b4_parse_assert_if([
      YYASSERT (!tname);
      YYASSERT (sizeof (T) <= S);
      tname = typeid (T).name ();])[
      return *new (buffer.raw) T;
    }
  \end{verbatim}

  \subsubsection{Introduction d'une nouvelle forme de pureté}

  Un parseur réentrant est un programme qui n'altère pas le cours de son
  exécution; autrement dit, il est consitué de code pur (sans effet de bord).
  La réentrance est importante lorsque l'on procède à des exécutions
  asynchrones; par exemple, un programme non-réentrant ne peut pas être invoqué
  de façon sûr à partir d'un \textit{signal handler}. Sur des systèmes à fils
  d'exécution multiple, un programme non-réentrant doit être protégé par des
  verrous.

  Normalement, les parseurs générés par Bison ne sont pas réentrants. Ceci est
  généralement suffisant pour les usages courants, et garanti la compatibilité
  arrière avec YACC (les interfaces standard de YACC étant non-réentrantes de
  par leur usage de variables statiquement allouées pour la communication avec
  l'analyseur lexical \texttt{yylex}, notamment \texttt{yylval} et
  \texttt{yylloc}).

  On peut également demander à Bison de générer un parseur pur, réentrant, avec
  la directive \texttt{\%define api.pure}. Ceci a pour effet que
  \texttt{yylval} et \texttt{yylloc} deviennent des variables locales à
  \texttt{yyparse}, et qu'une convention d'appel différente est utilisée pour
  \texttt{yylex}.

  Ce qui nous intéresse tout particulièrement, c'est le cas des parseurs qui
  traquent les \textit{locations}. L'interface de la fonction de gestion des
  erreurs devenait différente selon le squelette utilisé:

  \begin{verbatim}
  void yyerror (char const *msg);                // Yacc parsers.
  void yyerror (YYLTYPE *locp, char const *msg); // GLR parsers.
  \end{verbatim}

  On perdait la location sur les parseurs YACC\@. Or, ce comportement étaient
  en fait souhaité, pour des raisons de compatibilité.

  Nous avons donc introduit une troisième valeur possible pour la directive
  \texttt{\%define api.pure}: en plus des valeurs Booléennes, elle peut
  maintenant valoir \texttt{full}; le comportement est le même que pour
  \texttt{true} à l'exception près que les parseurs YACC avec locations peuvent
  maintenant avoir accès aux-dites locations dans leur gestion des erreurs,
  sans avoir à passer par des hacks étranges, comme c'était le cas.

  Voici un extrait des modifications apportées au M4:

  \begin{verbatim}
 b4_percent_define_default([[api.pure]], [[false]])
-b4_define_flag_if([pure])
-m4_define([b4_pure_flag],
-          [b4_percent_define_flag_if([[api.pure]], [[1]], [[0]])])
-
-# b4_yacc_pure_if(IF-TRUE, IF-FALSE)
-# ----------------------------------
-# Expand IF-TRUE, if %pure-parser and %parse-param, IF-FALSE otherwise.
-m4_define([b4_yacc_pure_if],
-[b4_pure_if([m4_ifset([b4_parse_param],
-                     [$1], [$2])],
-           [$2])])
-
+b4_percent_define_check_values([[[[api.pure]],
+                                 [[false]], [[true]], [[]], [[full]]]])
+
+m4_define([b4_pure_flag], [[0]])
+m4_case(b4_percent_define_get([[api.pure]]),
+        [false], [m4_define([b4_pure_flag], [[0]])],
+        [true],  [m4_define([b4_pure_flag], [[1]])],
+        [],      [m4_define([b4_pure_flag], [[1]])],
+        [full],  [m4_define([b4_pure_flag], [[2]])])
+
+m4_define([b4_pure_if],
+[m4_case(b4_pure_flag,
+         [0], [$2],
+         [1], [$1],
+         [2], [$1])])
+         [m4_fatal([invalid api.pure value: ]$1)])])
  \end{verbatim}

  On note le très élégant \texttt{b4\_define\_flag\_if([pure])}, qui est en fait
  une macro définissant des macros; en l'occurence la macro
  \texttt{b4\_percent\_define\_flag\_if}. Ces macros génériques vérifient la
  présence d'une directive \texttt{\%define} selon des règles Booléennes. D'un
  point de vue de généricité du code, c'était très bon. Ce que nous avons voulu
  faire, c'est spécialiser cette macro pour qu'elle donne non plus deux mais
  trois valeurs possibles, c'est ce qu'on voit dans la macro
  \texttt{b4\_pure\_flag}. Enfin, la macro \texttt{b4\_pure\_if} est un wrapper
  qui permet d'assurer la compatibilité arrière à moindre coût: à part pour le
  cas très précis des arguements de la fonction \texttt{errror}, cette nouvelle
  valeur n'a pas à être traitée différement d'un simple \texttt{true}.

  On apprécie dès ce genre de modification des squelettes la souplesse (et la
  généricité) apportée par GNU M4.

  \subsubsection{Réécriture des symboles}

  Voici à quoi ressemble l'affichage de debug de Bison:

  \begin{verbatim}
Starting parse
Entering state 0
Reading a token: Next token is token 'b' ()
Reducing stack by rule 4 (line 15):
-> $$ = nterm empty ()
Stack now 0
Entering state 4
Reducing stack by rule 3 (line 13):
   $1 = nterm empty ()
-> $$ = nterm a ()
Stack now 0
Entering state 3
Next token is token 'b' ()
Shifting token 'b' ()
Entering state 6
Reducing stack by rule 1 (line 10):
   $1 = nterm a ()
   $2 = token 'b' ()
-> $$ = nterm e ()
Stack now 0
Entering state 2
Reading a token: Now at end of input.
Shifting token $end ()
Entering state 5
Stack now 0 2 5
Cleanup: popping token $end ()
Cleanup: popping nterm e ()
  \end{verbatim}

  Il est question d'une pile. A chaque fois que l'on procède à un décalage de
  token, on ajoute un symbole correspondant à ce token sur la pile. Ce symbol
  contient plusieurs informations: la \textit{location} du symbole dans
  l'entrée utilisateur, et le type de token dont il s'agit, utilisé pour
  faire correspondre le haut de la pile à une règle existant de la grammaire,
  et procéder à une réduction. Lors d'une réduction, les symboles réduits dont
  dépilés, et on empile à la place un nouveau symbole, résultat de l'action que
  l'utilisateur a précisé pour cette règle.

  Cette pile était implémentée dans le squelette lalr1.cc avec une
  \texttt{std::deque} (\textit{double-ended queue}, liste doublement chaînée).
  Un choix plus logique (et plus cohérent avec les autres implémentation) étant
  un \texttt{std::vector}, nous avons voulu changer de conteneur.

  Les éléments dans un vecteur sont déplacés à chaque fois que le vecteur doit
  être dynamiquement redimensionné. Il y a alors une copie, faite par le
  constructeur par copie de notre objet \texttt{symbole}. Malheureusement, la
  construction ne pouvait se faire en un seul temps.

  Effectivement, les symboles ont une valeur sémantique, qui s'avère être typée
  selon les souhaits de l'utilisateur. On a vu que les directives Bison
  associaient les types aux tokens (et aux règles, c'est à dire aux états): il
  est donc nécessaire de savoir de quel token on est le symbole pour savoir
  quel type a notre valeur sémantique. Or, cette information est indispensable
  à la bonne construction de la valeur sémantique contenue dans le symbole, car
  implémentée avec (selon les cas) une union ou un variant.

  L'implémentation des symboles qui existait alors était un CRTP
  (\textit{curriously recurring template pattern}), à savoir, dans les grandes
  lignes:

  \begin{verbatim}
  template <typename Exact>
  struct symbol_base_type
  {};

  struct symbol_type : symbol_base_type<symbol_type>
  { /* Many things here*/ };

  struct stack_symbol_type : symbol_base_type<stack_symbol_type>
  { /* Many things here*/ };
  \end{verbatim}

  \begin{figure}[H]
    \begin{center}
      \includegraphics[scale=1]{images/crtp}
      \caption{Le design pattern CRTP.}
    \end{center}
  \end{figure}

  Qui a été remplacé par:

  \begin{verbatim}
  template <typename Base>
  basic_symbol : Base
  {};

  typedef basic_symbol<by_type> symbol_type;
  typedef basic_symbol<by_state> stack_symbol_type;
  \end{verbatim}

  Pourquoi?

  Il y a deux types de symboles:
  \begin{itemize}
    \item les symboles \textit{externes}, \texttt{symbol\_type}: ils ont un
      type, une valeur et une location, et sont retournés par \texttt{yylex}.
    \item les symbols \textit{internes}, \texttt{stack\_symbol\_type}, groupent
      un numéro d'état, une valeur et une location, et sont stockés dans la
      pile du parseur. Le type du symbole est calculé à partir de l'état.
  \end{itemize}


  Le template de classe \texttt{symbol\_base\_type<Exact>} factorise le code
  commun à \texttt{stack\_symbol\_type} and \texttt{symbol\_type}.  Il utilise
  un CRTP afin de toujours pouvoir \textit{downcast} vers le type exact.
  \texttt{symbol\_base\_type} implémente la valeur et la location, et délègue
  la gestion du typage à son paramètre.

  En tenant de généraliser le support de \texttt{variant}, on a heurté un mur:
  comme \texttt{stack\_symbol\_type} et \texttt{symbol\_type} héritent tout
  deux de \texttt{symbol\_base\_type}, le type/état est définit
  \underline{après} la valeur et la location. En C++, l'ordre de définition des
  membres définit l'ordre dans lequel ils sont initialisés, et les choses vont
  du coup à reculons: la valeur est initialisée \underline{avant} le type. Ceci
  est impossible, le type est nécessaire à l'initialisation de la valeur.

  Ainsi, il nous a fallu trouver un nouveau moyen de factoriser le code commun,
  un qui garantie le bon ordre des membres.

  L'idée est simple, on définit deux classes (mères) qui implémentent le type
  du type (\texttt{by\_type}\footnote{ces noms furent choisis par similitude à
  basic\_string et basic\_ostream} le code directement par type,
  \texttt{by\_state} en revanche le fait par numéro d'état). On définit alors
  \texttt{basic\_symbol<Base>} le template de classe qui donne la valeur et la
  location, et on le fait dériver d'un paramètre, \texttt{by\_type} ou
  \texttt{by\_state}.

  Une fuite de mémoire liée à l'introduction de ces changements attira alors
  notre attention.

  \subsubsection{Résolution de la fuite de l'api.token.constructor}

  Traditionnellement, l'analyseur lexical \texttt{yylex} retourne un entier,
  qui correspond à la valeur du token reconnu (NUM, PUNCT, etc.). La valeur
  sémantique associée, elle, est stockée dans l'argument \texttt{yylval}, de la
  façon suivante:

  \begin{verbatim}
  // update yylloc
  yylval->build< std::string > (yytext);
  return parser::token::TOK_TEXT;
  \end{verbatim}

  Les versions récentes de Bison introduisent la directive \texttt{\%define
  api.token.constructor} qui change l'interface de \texttt{yylex}, qui renvoie
  désormais un \texttt{symbol\_type}:

  \begin{verbatim}
  return parser::make_TOK_TEXT (yytext, /* location here */);
  \end{verbatim}

  Une bug vicieux et silencieux s'était caché dans les fonctions
  \texttt{make\_}, que voici:

  \begin{verbatim}
  parser::symbol_type
  parser::make_TEXT (const ::std::string& v)
  {
    return symbol_type (token::TOK_TEXT, v);
  }
  \end{verbatim}

  Le constructeur de \texttt{symbol\_type} ne prend pas de
  \texttt{::std::string\&} en argument, mais un \texttt{const variant}.
  Cependant, comme il y a un constructeur de \texttt{variant} qui prend un
  \texttt{::std::string\&}, ceci construisait implicitement un variant
  initialisé. Comme le variant passé en argument au constructeur de
  \texttt{symbol\_type} est constant, il n'est pas libéré; ce constructeur
  n'est normalement jamais appelé hors de notre contrôle, et doit
  systématiquement être libéré explicitement, autrement que par le destructeur.
  Le variant temporaire n'était donc jamais vidé, son contenu fuyait la
  mémoire. Une première solution a été de rétablir une construction en deux
  temps (sans variable temporaire):

  \begin{verbatim}
  symbol_type res (token::TOK_TEXT);
  res.value.build< ::std::string&> (v);
  return res;
  \end{verbatim}

  La solution retenue tente d'éviter tout reproduction de ce bug, en ajoutant
  les constructeurs qui manquaient. Maintenant, les symboles s'auto-gèrent:
  leur construction peut se paramétrer par une valeur sémantique directement
  (non-wrappée par un variant).

  Les constructeurs sont générés en M4. Voici la génération de leur définition
  (leur déclaration est générée d'une façon très semblable):
  \begin{verbatim}
Dans basic_symbol:

  // Implementation of basic_symbol constructor for each type.
]b4_type_foreach([b4_basic_symbol_constructor_define])[

En aval, parmis les macros m4:

# b4_basic_symbol_constructor_define
# ----------------------------------
# Generate a constructor implementation for basic_symbol from given type.
m4_define([b4_basic_symbol_constructor_define],
[[
  template <typename Base>
  ]b4_parser_class_name[::basic_symbol<Base>::basic_symbol (]b4_join(
          [typename Base::kind_type t],
          b4_symbol_if([$1], [has_type], const b4_symbol([$1], [type])[ v]),
          b4_locations_if([const location_type& l]))[)
    : Base (t)
    , value (]b4_symbol_if([$1], [has_type], [v])[)]b4_locations_if([
    , location (l)])[
  {}
]])
  \end{verbatim}

  \cleardoublepage

  \section{Conclusion}

  Chaque modification apportée à Bison a pour moi été l'occasion d'acquérir de
  nouvelles compétences et de nouveaux savoir-faire, qui m'ont permis d'aborder
  des travaux de plus en plus intéressants.

  Les deux premiers mois du stage ont été consacrés à du développement en C, un
  langage qui m'étais déjà bien familier. Cependant, j'ai vite découvert que
  mes connaissances de l'outil de gestion de versions Git étaient en fait
  bien plus faibles que ce que je m'imaginais, et arriver sur un projet de
  cette envergure a été une parfaite occasion pour me perfectionner.

  Lorsque le travail sur l'affichage des réductions dans les graphes m'a été
  proposé, Akim m'a demandé si le langage Dot m'était familier; c'était le cas.
  Je pensais tout de même que j'aurais des difficultés sur cet aspect là de la
  tâche -- loin de moi à l'époque l'idée que ce qui me donnerait tant de mail
  serait l'algorithme qui se cachait derrière, et encore plus l'idée que je
  bloquerais sur des détails d'implémentation de C! Et quelle ne fut pas
  surprise lorsque vint le travail sur les fichiers XSL\ldots Oui, ce travail
  sur les graphes fut difficile, mais c'est avec joie que je l'ai vu apprécié.

  La seconde partie de ce stage a porté sur divers travaux sur les squelettes,
  fichiers riches en GNU M4. Ce langage ne faisait pas du tout parti de ma
  panoplie à mon arrivée, et il faut bien reconnaître qu'il est intimidant, à
  sa manière. Cependant, les petites manipulations nécessaires sur la suite de
  tests pendant la première partie du stage ont suffit à me donner confiance,
  et à me donner de l'intérêt pour cet enchevêtrement de crochets et de
  parenthèses.

  En fin de compte, chaque phase du stage m'a permis de découvrir de nouvelles
  choses, qui à chaque fois m'ont plus. Travailler sur GNU Bison est un
  plaisir, ce projet a des facettes multiples, et toutes très intéressantes; et
  travailler avec Akim Demaille est un plaisir aussi.




  \cleardoublepage
  \section{Annexes}

  \subsection{Liste des commits appliqués à Bison}
  Voici la sortie de la commande \texttt{git shortlog --author=ranquet}
  \begin{adjustwidth}{-2cm}{-2cm}
    \include{commits}
  \end{adjustwidth}

  \cleardoublepage
  \cleardoublepage
\end{document}

